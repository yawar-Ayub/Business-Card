{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOclhorFKB48AijhTrlwiNl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yawar-Ayub/Business-Card/blob/main/Untitled91.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh1SywHYPZl9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "  e_x = np.exp(x-np.max(x,axis=-1,keepdims=True))\n",
        "  return e_x / np.sum(e_x,axis =-1,keepdims=True)"
      ],
      "metadata": {
        "id": "My3YZT6Rii2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = [[1,2,3],[4,5,6]]\n",
        "softmax(arr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndNLxd8dijIW",
        "outputId": "3ace19b0-e820-4f23-afe9-c2da0c7c92ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.09003057, 0.24472847, 0.66524096],\n",
              "       [0.09003057, 0.24472847, 0.66524096]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_self_attention(sentence):\n",
        "  words = sentence.split()\n",
        "  vocab = {word : np.random.rand(4) for word in words}\n",
        "  print(words,\"\\n\",vocab)\n",
        "  embeddings = np.array([vocab[word] for word in words])\n",
        "  print(\"embeddings\",embeddings)\n",
        "\n",
        "  #step\n",
        "  dk = embeddings.shape[-1]\n",
        "  W_Q = np.random.rand(dk,dk)\n",
        "  W_K = np.random.rand(dk,dk)\n",
        "  W_V = np.random.rand(dk,dk)\n",
        "  print(W_K)\n",
        "  K = embeddings @ W_K\n",
        "  Q = embeddings @ W_Q\n",
        "  V = embeddings @ W_V\n",
        "  print(\"K : \",K,\"\\n\",\"Q : \",Q,\"\\n V : \",V)\n",
        "  print(\"k.shape : \",K.shape,\" Q.shape : \",Q.shape,\"V.shape : \",V.shape)\n",
        "\n",
        "  scores = Q @ K.T /np.sqrt(dk)\n",
        "  attention_weights = softmax(scores)\n",
        "  print('attention weights : ',attention_weights)\n",
        "  print(\"attention_weights sahpe: \",attention_weights.shape,np.sum(attention_weights[1]))\n",
        "  weighted_sum = attention_weights @ V\n",
        "\n",
        "  print(\"weighted sum is \",weighted_sum, weighted_sum.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Gi_qyzrDi_F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"this is yawar learning nlp and it is going to be fun\"\n",
        "simple_self_attention(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgcDvzc6kDs5",
        "outputId": "94b2e785-29ab-4dfb-c0d3-7a62d5452292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'is', 'yawar', 'learning', 'nlp', 'and', 'it', 'is', 'going', 'to', 'be', 'fun'] \n",
            " {'this': array([0.11429537, 0.63316176, 0.76909753, 0.70420382]), 'is': array([0.23255206, 0.7630284 , 0.7297464 , 0.92714859]), 'yawar': array([0.22239901, 0.31984221, 0.73809012, 0.34584149]), 'learning': array([0.95285478, 0.74676652, 0.78122021, 0.37506121]), 'nlp': array([0.56008433, 0.6179122 , 0.96440608, 0.3374152 ]), 'and': array([0.52746669, 0.67763333, 0.57201271, 0.31927112]), 'it': array([0.38070278, 0.19613596, 0.82973239, 0.1858838 ]), 'going': array([0.36998417, 0.31459357, 0.75050973, 0.2176295 ]), 'to': array([0.7184702 , 0.56909591, 0.26059951, 0.88316627]), 'be': array([0.87500144, 0.87656194, 0.9282738 , 0.15377658]), 'fun': array([0.45466098, 0.48525424, 0.98080635, 0.72317904])}\n",
            "embeddings [[0.11429537 0.63316176 0.76909753 0.70420382]\n",
            " [0.23255206 0.7630284  0.7297464  0.92714859]\n",
            " [0.22239901 0.31984221 0.73809012 0.34584149]\n",
            " [0.95285478 0.74676652 0.78122021 0.37506121]\n",
            " [0.56008433 0.6179122  0.96440608 0.3374152 ]\n",
            " [0.52746669 0.67763333 0.57201271 0.31927112]\n",
            " [0.38070278 0.19613596 0.82973239 0.1858838 ]\n",
            " [0.23255206 0.7630284  0.7297464  0.92714859]\n",
            " [0.36998417 0.31459357 0.75050973 0.2176295 ]\n",
            " [0.7184702  0.56909591 0.26059951 0.88316627]\n",
            " [0.87500144 0.87656194 0.9282738  0.15377658]\n",
            " [0.45466098 0.48525424 0.98080635 0.72317904]]\n",
            "[[0.00623682 0.74538695 0.80871277 0.36216993]\n",
            " [0.49160928 0.72849029 0.08480497 0.87951978]\n",
            " [0.39332529 0.57758052 0.84388364 0.81599107]\n",
            " [0.60499454 0.1358005  0.41323731 0.95747056]]\n",
            "K :  [[1.040526   1.08629345 1.0861595  1.90010378]\n",
            " [1.22450978 1.2765946  1.25172986 2.23850597]\n",
            " [0.65816618 0.87204719 0.97275791 1.29526178]\n",
            " [0.90724378 1.75640875 1.64816359 1.99847005]\n",
            " [0.89072417 1.4704659  1.45862844 1.85632358]\n",
            " [0.7545649  1.26055666 1.09868264 1.55947446]\n",
            " [0.53761005 0.9311344  1.10152421 1.16541704]\n",
            " [1.22450978 1.2765946  1.25172986 2.23850597]\n",
            " [0.58382376 0.96799372 1.04916553 1.23147148]\n",
            " [0.92106495 1.22057078 1.21417111 1.81899199]\n",
            " [0.89453076 1.84781731 1.62886295 1.99255246]\n",
            " [1.06468643 1.35710407 1.5353731  2.08420709]] \n",
            " Q :  [[0.91209707 1.14527954 1.36511919 1.37411385]\n",
            " [1.09829977 1.22485796 1.62822477 1.65616772]\n",
            " [0.62225162 0.94550938 0.93531547 1.07832608]\n",
            " [0.93068682 1.39104186 1.63313571 2.15174244]\n",
            " [0.85162185 1.39661544 1.43431353 1.77177927]\n",
            " [0.70174828 1.05058073 1.25525755 1.52189145]\n",
            " [0.5615111  0.99045654 0.85818681 1.13493004]\n",
            " [1.09829977 1.22485796 1.62822477 1.65616772]\n",
            " [0.57854399 0.97900494 0.92710717 1.17318271]\n",
            " [0.963317   0.79540315 1.40916095 1.67313683]\n",
            " [0.85766108 1.55999501 1.6553576  2.16912767]\n",
            " [1.04617919 1.34941441 1.51037546 1.74752821]] \n",
            " V :  [[0.59588724 1.76182707 1.38146607 0.84071217]\n",
            " [0.79637048 2.12486294 1.5427162  0.90510741]\n",
            " [0.5288768  1.31146248 1.1082292  0.74109301]\n",
            " [1.35714262 2.44143734 1.88488631 1.11175838]\n",
            " [0.96630731 2.05941451 1.73735002 1.08765376]\n",
            " [0.86516591 1.76376465 1.40179167 0.79230493]\n",
            " [0.63167956 1.31267233 1.15556305 0.81587889]\n",
            " [0.79637048 2.12486294 1.5427162  0.90510741]\n",
            " [0.63939863 1.36476588 1.17649461 0.78263318]\n",
            " [1.1336619  2.04778797 1.20203751 0.66324907]\n",
            " [1.27178818 2.41917608 2.05536964 1.19947275]\n",
            " [0.94863032 2.14833147 1.64121453 1.08169391]]\n",
            "k.shape :  (12, 4)  Q.shape :  (12, 4) V.shape :  (12, 4)\n",
            "attention weights :  [[0.06536014 0.11197832 0.02966262 0.14174768 0.09517724 0.05059266\n",
            "  0.02900372 0.11197832 0.03054656 0.06899073 0.14596333 0.11899867]\n",
            " [0.06201806 0.11674879 0.02436288 0.14895165 0.0943884  0.04493601\n",
            "  0.02357889 0.11674879 0.02503755 0.06543828 0.15324049 0.12455021]\n",
            " [0.07101    0.10668844 0.03899491 0.12825422 0.09448265 0.05905306\n",
            "  0.03824711 0.10668844 0.03992488 0.07408897 0.13176977 0.11079754]\n",
            " [0.05897758 0.12083284 0.02022428 0.15539318 0.09291568 0.04081476\n",
            "  0.01924652 0.12083284 0.02075493 0.06231636 0.16101416 0.12667688]\n",
            " [0.06135069 0.11516482 0.02421649 0.15110678 0.09457791 0.04577561\n",
            "  0.02343611 0.11516482 0.02504472 0.06533016 0.15716886 0.12166302]\n",
            " [0.06671332 0.11288228 0.0306391  0.13882661 0.09464215 0.0514323\n",
            "  0.0297559  0.11288228 0.03137485 0.06994054 0.14261726 0.11829341]\n",
            " [0.07107125 0.10698155 0.03879711 0.12839287 0.09433558 0.05925061\n",
            "  0.03791441 0.10698155 0.03970895 0.07410939 0.13231083 0.1101459 ]\n",
            " [0.06201806 0.11674879 0.02436288 0.14895165 0.0943884  0.04493601\n",
            "  0.02357889 0.11674879 0.02503755 0.06543828 0.15324049 0.12455021]\n",
            " [0.07056936 0.10757893 0.03785463 0.12957815 0.09447112 0.05827497\n",
            "  0.03701515 0.10757893 0.03875324 0.0736651  0.13334414 0.11131627]\n",
            " [0.06833867 0.1201247  0.02905558 0.13496998 0.09284372 0.0484198\n",
            "  0.02757046 0.1201247  0.02913766 0.06959394 0.13655576 0.12326503]\n",
            " [0.05689356 0.11822295 0.01930286 0.16054243 0.09344736 0.04026204\n",
            "  0.01854861 0.11822295 0.02003072 0.06110973 0.16767243 0.12574435]\n",
            " [0.06155766 0.11737011 0.02360087 0.15031828 0.0940521  0.04469204\n",
            "  0.02268894 0.11737011 0.02426648 0.06496727 0.15571582 0.12340032]]\n",
            "attention_weights sahpe:  (12, 12) 0.9999999999999999\n",
            "weighted sum is  [[0.97568577 2.09104214 1.61981604 0.977958  ]\n",
            " [0.98615496 2.1120803  1.63537171 0.98616566]\n",
            " [0.95568256 2.05472028 1.59207152 0.96336123]\n",
            " [0.99530232 2.12952657 1.64866232 0.99289742]\n",
            " [0.98904036 2.11401559 1.63801791 0.98730244]\n",
            " [0.97185036 2.08624866 1.61524863 0.97540689]\n",
            " [0.95604313 2.05541784 1.59255269 0.96346464]\n",
            " [0.98615496 2.1120803  1.63537171 0.98616566]\n",
            " [0.95783416 2.05890795 1.59510707 0.96484687]\n",
            " [0.96774195 2.08876613 1.61359517 0.97442294]\n",
            " [1.00175159 2.13597557 1.65561003 0.99664435]\n",
            " [0.988254   2.11545674 1.63815669 0.9873822 ]] (12, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(attent)"
      ],
      "metadata": {
        "id": "Ki8gTEiqkVw7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}